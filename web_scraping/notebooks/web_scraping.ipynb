{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Pricing Web Scraper\n",
    "\n",
    "This notebook is designed to automate the collection of house pricing data from multiple real estate websites. The goal of this project is to build a comprehensive dataset that can be used for further analysis and to train machine learning models to predict house prices based on various features.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Setup](#setup)\n",
    "3. [Web Scraping](#web-scraping)\n",
    "4. [Save Data](#save-data)\n",
    "5. [Conclusion](#conclusion)\n",
    "\n",
    "## Introduction <a name=\"introduction\"></a>\n",
    "In this section, we will describe the purpose of the notebook and the approach we will take to scrape the data from multiple websites.\n",
    "\n",
    "## Setup <a name=\"setup\"></a>\n",
    "In this section, we will import the necessary libraries and set up any configurations required for web scraping.\n",
    "\n",
    "```python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "BASE_DOMAIN = os.getenv(\"BASE_DOMAIN\")\n",
    "BASE_PATH = os.getenv(\"BASE_PATH\")\n",
    "HEADERS_USER_AGENT = os.getenv(\"HEADERS_USER_AGENT\")\n",
    "headers = {\"User-Agent\": HEADERS_USER_AGENT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping <a name=\"web-scraping\"></a>\n",
    "In this section, we will scrape the house listings from multiple websites. The process will continue until no more pages are available or a set page limit is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "Found 50 links on page 1\n",
      "Scraping page 2\n",
      "Found 50 links on page 2\n",
      "Scraping page 3\n",
      "Found 50 links on page 3\n",
      "Scraping page 4\n",
      "Found 50 links on page 4\n",
      "Scraping page 5\n",
      "Found 50 links on page 5\n",
      "Scraping page 6\n",
      "Found 50 links on page 6\n",
      "Scraping page 7\n",
      "Found 50 links on page 7\n",
      "Scraping page 8\n",
      "Found 50 links on page 8\n",
      "Scraping page 9\n",
      "Found 50 links on page 9\n",
      "Scraping page 10\n",
      "Found 50 links on page 10\n",
      "Scraping page 11\n",
      "Found 50 links on page 11\n",
      "Scraping page 12\n",
      "Found 50 links on page 12\n",
      "Scraping page 13\n",
      "Found 50 links on page 13\n",
      "Scraping page 14\n",
      "Found 50 links on page 14\n",
      "Scraping page 15\n",
      "Found 50 links on page 15\n",
      "Scraping page 16\n",
      "Found 50 links on page 16\n",
      "Scraping page 17\n",
      "Found 50 links on page 17\n",
      "Scraping page 18\n",
      "Found 50 links on page 18\n",
      "Scraping page 19\n",
      "Found 50 links on page 19\n",
      "Scraping page 20\n",
      "Found 50 links on page 20\n",
      "Scraping page 21\n",
      "Found 50 links on page 21\n",
      "Scraping page 22\n",
      "Found 50 links on page 22\n",
      "Scraping page 23\n",
      "Found 50 links on page 23\n",
      "Scraping page 24\n",
      "Found 50 links on page 24\n",
      "Scraping page 25\n",
      "Found 50 links on page 25\n",
      "Scraping page 26\n",
      "Found 50 links on page 26\n",
      "Scraping page 27\n",
      "Found 50 links on page 27\n",
      "Scraping page 28\n",
      "Found 50 links on page 28\n",
      "Scraping page 29\n",
      "Found 50 links on page 29\n",
      "Scraping page 30\n",
      "Found 50 links on page 30\n",
      "Scraping page 31\n",
      "Found 50 links on page 31\n",
      "Scraping page 32\n",
      "Found 50 links on page 32\n",
      "Scraping page 33\n",
      "Found 50 links on page 33\n",
      "Scraping page 34\n",
      "Found 50 links on page 34\n",
      "Scraping page 35\n",
      "Found 50 links on page 35\n",
      "Scraping page 36\n",
      "Found 50 links on page 36\n",
      "Scraping page 37\n",
      "Found 50 links on page 37\n",
      "Scraping page 38\n",
      "Found 50 links on page 38\n",
      "Scraping page 39\n",
      "Found 50 links on page 39\n",
      "Scraping page 40\n",
      "Found 50 links on page 40\n"
     ]
    }
   ],
   "source": [
    "# Function to get advertisement links from a page\n",
    "def get_ad_links(soup):\n",
    "    links = []\n",
    "    listings = soup.find_all(\"div\", {\"class\": \"_637fa00f\"})\n",
    "    if not listings:\n",
    "        print(\"No listings found on the page.\")\n",
    "    for listing in listings:\n",
    "        try:\n",
    "            link_tag = listing.find(\"a\", href=True)\n",
    "            full_link = BASE_DOMAIN + link_tag[\"href\"]\n",
    "            links.append(full_link)\n",
    "        except AttributeError:\n",
    "            continue\n",
    "    return links\n",
    "\n",
    "\n",
    "# Scrape links from multiple pages\n",
    "ad_links = []\n",
    "page_number = 1\n",
    "while page_number <= 40:\n",
    "    print(f\"Scraping page {page_number}\")\n",
    "    url = f\"{BASE_DOMAIN}{BASE_PATH}{page_number}\"\n",
    "    r = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    links_on_page = get_ad_links(soup)\n",
    "    ad_links.extend(links_on_page)\n",
    "    print(f\"Found {len(links_on_page)} links on page {page_number}\")\n",
    "    page_number += 1\n",
    "    time.sleep(1)\n",
    "\n",
    "if not ad_links:\n",
    "    print(\"No links were found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data <a name=\"save-data\"></a>\n",
    "In this section, we will save the collected data into CSV files for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save links to CSV\n",
    "save_path = \"../data/ad_links.csv\"\n",
    "with open(save_path, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Ad URL\"])\n",
    "    for link in ad_links:\n",
    "        writer.writerow([link])\n",
    "\n",
    "\n",
    "# Function to scrape data from an individual ad page\n",
    "def scrape_ad_page(ad_url):\n",
    "    r = requests.get(ad_url, headers=headers)\n",
    "    ad_soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    try:\n",
    "        price = ad_soup.find(\"span\", {\"aria-label\": \"Price\"}).text.strip()\n",
    "        area = ad_soup.find(\"span\", {\"aria-label\": \"Area\"}).text.strip()\n",
    "        bedrooms = ad_soup.find(\"span\", {\"aria-label\": \"Bedrooms\"}).text.strip()\n",
    "        bathrooms = ad_soup.find(\"span\", {\"aria-label\": \"Bathrooms\"}).text.strip()\n",
    "        location = ad_soup.find(\"span\", {\"aria-label\": \"Location\"}).text.strip()\n",
    "        return [ad_url, price, area, bedrooms, bathrooms, location]\n",
    "    except AttributeError:\n",
    "        print(f\"Missing data on page: {ad_url}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Load links from the CSV file and scrape data from each ad\n",
    "scraped_data = []\n",
    "links_file_path = (\n",
    "    \"../data/ad_links.csv\"\n",
    ")\n",
    "with open(links_file_path, \"r\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # Skip the header row\n",
    "    ad_links = [row[0] for row in reader]\n",
    "\n",
    "for ad_link in ad_links:\n",
    "    ad_data = scrape_ad_page(ad_link)\n",
    "    if ad_data:\n",
    "        scraped_data.append(ad_data)\n",
    "    time.sleep(1)  # To avoid overloading the server\n",
    "\n",
    "# Save scraped data to CSV\n",
    "save_path = (\n",
    "    \"../data/ad_details.csv\"\n",
    ")\n",
    "headers = [\"Ad URL\", \"Price\", \"Area (m^2)\", \"Bedrooms\", \"Bathrooms\", \"Location\"]\n",
    "with open(save_path, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(headers)\n",
    "    writer.writerows(scraped_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion <a name=\"conclusion\"></a>\n",
    "In this notebook, we successfully scraped house pricing data from multiple websites. The data has been saved into a CSV file for further analysis. This dataset can now be used for training machine learning models to predict house prices based on various features.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
