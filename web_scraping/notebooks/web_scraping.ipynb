{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Pricing Data Scraper\n",
    "This notebook demonstrates how to scrape house pricing data from multiple websites dynamically. The configuration for each website is stored in a separate JSON file to keep the main code clean and professional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration\n",
    "The configuration file contains the details required for scraping each website, such as base URLs and labels for the required data fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-27 18:00:27,723 - INFO - Configuration loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "config_path = Path(\"../config.json\")\n",
    "with config_path.open() as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "logging.info(\"Configuration loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Scraping\n",
    "We define functions to extract data from individual listings and to scrape data from a given website configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get property links from a page\n",
    "def get_property_links(base_url, page_num, listing_div_class):\n",
    "    url = f\"{base_url}{page_num}\"\n",
    "    logging.info(f\"Fetching property links from page {page_num}...\")\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    listing_divs = soup.find_all(\"div\", class_=listing_div_class)\n",
    "    links = [div.find(\"a\")[\"href\"] for div in listing_divs if div.find(\"a\")]\n",
    "    logging.info(f\"Found {len(links)} property links on page {page_num}.\")\n",
    "    return links\n",
    "\n",
    "\n",
    "# Function to extract data from a single property page\n",
    "def extract_property_data(url, base_domain, labels, neighborhoods):\n",
    "    full_url = base_domain + url\n",
    "    logging.info(f\"Fetching property data from page...\")\n",
    "    response = requests.get(full_url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    data = {}\n",
    "\n",
    "    # Extract price\n",
    "    price_elem = soup.find(\"span\", {\"aria-label\": labels[\"price\"][\"aria_label\"]})\n",
    "    data[\"price\"] = price_elem.text.strip() if price_elem else \"N/A\"\n",
    "\n",
    "    # Extract area\n",
    "    area_label = soup.find(\"span\", text=labels[\"area\"][\"text_label\"])\n",
    "    data[\"area\"] = area_label.find_next(\"span\").text.strip() if area_label else \"N/A\"\n",
    "\n",
    "    # Extract bedrooms\n",
    "    bedrooms_label = soup.find(\"span\", text=labels[\"bedrooms\"][\"text_label\"])\n",
    "    data[\"bedrooms\"] = (\n",
    "        bedrooms_label.find_next(\"span\").text.strip() if bedrooms_label else \"N/A\"\n",
    "    )\n",
    "\n",
    "    # Extract bathrooms\n",
    "    bathrooms_label = soup.find(\"span\", text=labels[\"bathrooms\"][\"text_label\"])\n",
    "    data[\"bathrooms\"] = (\n",
    "        bathrooms_label.find_next(\"span\").text.strip() if bathrooms_label else \"N/A\"\n",
    "    )\n",
    "\n",
    "    # Extract location\n",
    "    location_elem = soup.find(\"span\", {\"aria-label\": labels[\"location\"][\"aria_label\"]})\n",
    "    location = location_elem.text.strip() if location_elem else \"N/A\"\n",
    "\n",
    "    # Ensure the location is in Alexandria\n",
    "    if any(neighborhood in location for neighborhood in neighborhoods):\n",
    "        if \"Alexandria\" not in location:\n",
    "            location += \", Alexandria\"\n",
    "    else:\n",
    "        location = \"N/A\"\n",
    "\n",
    "    data[\"location\"] = location\n",
    "\n",
    "    logging.info(f\"Extracted data: {data}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Data\n",
    "We define a function to manage the entire scraping process for each website, collect the data, and store it in a DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape data from a given website configuration\n",
    "def scrape_website(config, max_pages=40):\n",
    "    all_links = []\n",
    "    base_url = config[\"BASE_DOMAIN\"] + config[\"BASE_PATH\"]\n",
    "\n",
    "    # Collect all property links from the first 40 pages\n",
    "    for page in range(1, max_pages + 1):\n",
    "        logging.info(f\"Collecting links from page {page}...\")\n",
    "        links = get_property_links(base_url, page, config[\"LISTING_DIV_CLASS\"])\n",
    "        if not links:\n",
    "            logging.info(\"No more links found, stopping.\")\n",
    "            break\n",
    "        all_links.extend(links)\n",
    "        sleep(1)  # To avoid getting blocked by the website\n",
    "\n",
    "    # Collect detailed data from each property link\n",
    "    listings = []\n",
    "    for link in all_links:\n",
    "        logging.info(f\"Extracting data from a property link...\")\n",
    "        data = extract_property_data(\n",
    "            link, config[\"BASE_DOMAIN\"], config[\"FIELDS\"], config[\"NEIGHBORHOODS\"]\n",
    "        )\n",
    "        if data[\"location\"] != \"N/A\":\n",
    "            listings.append(data)\n",
    "        sleep(1)  # To avoid getting blocked by the website\n",
    "\n",
    "    return pd.DataFrame(listings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Scraping Process\n",
    "We loop through each website in the configuration, scrape the data, and combine it into a single DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape data for each website in the configuration\n",
    "all_data = []\n",
    "for website_name, website_config in config[\"websites\"].items():\n",
    "    logging.info(f\"Scraping data from a website configuration...\")\n",
    "    data = scrape_website(website_config)\n",
    "    all_data.append(data)\n",
    "\n",
    "# Combine all data into a single DataFrame\n",
    "all_data_df = pd.concat(all_data, ignore_index=True)\n",
    "logging.info(\"Data scraping completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Display Data\n",
    "We save the combined data to a CSV file and display the first few rows for verification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data to a CSV file\n",
    "output_path = Path(\"../data/house_pricing_data.csv\")\n",
    "all_data_df.to_csv(output_path, index=False)\n",
    "logging.info(f\"Data saved to {output_path}\")\n",
    "\n",
    "# Display the first few rows of the data\n",
    "all_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Existing Data and Check for New Entries\n",
    "We load the existing dataset, check for new entries, and add only the new entries to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing data\n",
    "existing_data_path = Path(\"../data/house_pricing_data.csv\")\n",
    "if existing_data_path.exists():\n",
    "    existing_data_df = pd.read_csv(existing_data_path)\n",
    "else:\n",
    "    existing_data_df = pd.DataFrame()\n",
    "\n",
    "# Scrape data for each website in the configuration\n",
    "all_data = []\n",
    "for website_name, website_config in config[\"websites\"].items():\n",
    "    logging.info(f\"Scraping data from a website configuration...\")\n",
    "    data = scrape_website(website_config)\n",
    "    all_data.append(data)\n",
    "\n",
    "# Combine all new data into a single DataFrame\n",
    "new_data_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Ensure that \"Alexandria\" is added to locations if missing\n",
    "new_data_df[\"location\"] = new_data_df[\"location\"].apply(\n",
    "    lambda x: x if \"Alexandria\" in x else x + \", Alexandria\"\n",
    ")\n",
    "\n",
    "# Check for new entries by comparing with existing data\n",
    "if not existing_data_df.empty:\n",
    "    combined_df = pd.concat([existing_data_df, new_data_df]).drop_duplicates(\n",
    "        subset=[\"price\", \"area\", \"bedrooms\", \"bathrooms\", \"location\"], keep=\"first\"\n",
    "    )\n",
    "else:\n",
    "    combined_df = new_data_df\n",
    "\n",
    "# Save the combined data to a CSV file\n",
    "combined_df.to_csv(existing_data_path, index=False)\n",
    "logging.info(f\"Data saved to CSV file: {existing_data_path}\")\n",
    "\n",
    "# Display the first few rows of the data\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Duplicates from the Dataset\n",
    "We check for and remove duplicate rows from the dataset, then save the cleaned dataset back to the CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = \"../data/house_pricing_data.csv\"\n",
    "house_pricing_data = pd.read_csv(file_path)\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicates = house_pricing_data[house_pricing_data.duplicated()]\n",
    "\n",
    "# Display the number of duplicate rows and the duplicate rows themselves\n",
    "num_duplicates = duplicates.shape[0]\n",
    "print(f\"Number of duplicate rows: {num_duplicates}\")\n",
    "print(\"Duplicate rows:\")\n",
    "print(duplicates)\n",
    "\n",
    "# Remove duplicates from the dataset\n",
    "house_pricing_data_cleaned = house_pricing_data.drop_duplicates()\n",
    "\n",
    "# Save the cleaned dataset to the original CSV file\n",
    "cleaned_file_path = \"../data/house_pricing_data.csv\"\n",
    "house_pricing_data_cleaned.to_csv(cleaned_file_path, index=False)\n",
    "logging.info(f\"Cleaned data saved to {cleaned_file_path}\")\n",
    "\n",
    "# Display the first few rows of the cleaned data\n",
    "house_pricing_data_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "The notebook successfully scraped house pricing data from multiple websites, validated the data, and ensured it meets expected standards. The data has been saved to a CSV file for further analysis and building deep learning models to predict house prices.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
